{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yzg7lrQ_v2bP"
   },
   "source": [
    "### QUALIFICAÇÃO\n",
    "\n",
    "__Bases de dados utilizada:__  \n",
    "- 20-Newsgroups: composta por 18.846 documentos em 20 classes  [(link)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html).  \n",
    "- Safety Alerts: elaborada pelo autor composta por 100 documentos em 10 classes.\n",
    "\n",
    "__Classificadores testados:__  \n",
    "- Regressão Logística  \n",
    "- Naive Bayes  \n",
    "- SVM  \n",
    "- KNN  \n",
    "- Árvore de Decisão  \n",
    "- Random Forest   \n",
    "- Rede Neural  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do dataset Movie Reviews da biblioteca NLTK\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "safety_alerts_dir = r'C:\\Users\\wander\\safetyrank\\app\\static\\safety-alerts-database'\n",
    "safety_alerts_data = load_files(safety_alerts_dir, shuffle=True)\n",
    "safety_alerts_X, safety_alerts_y = safety_alerts_data.data, safety_alerts_data.target\n",
    "\n",
    "safety_alerts_X_train, safety_alerts_X_test, safety_alerts_y_train, safety_alerts_y_test = train_test_split(safety_alerts_X, safety_alerts_y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "JZ1VzqxHv2bX",
    "outputId": "40e99879-1e08-496a-aeac-c42755c987d8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Carregamento do dataset 20 Newsgroups\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "newsgroups_X_train = newsgroups_train.data\n",
    "newsgroups_y_train = newsgroups_train.target\n",
    "newsgroups_X_test = newsgroups_test.data\n",
    "newsgroups_y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQMT54Kcv2bl",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Carregamento das bibliotecas comuns do sklearn\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezuoem1bv2bu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Regressão Logística\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_clf_logistic_regression = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression(penalty='l2', \n",
    "                                                dual=False, \n",
    "                                                tol=0.0001, \n",
    "                                                C=1.0, \n",
    "                                                fit_intercept=True, \n",
    "                                                intercept_scaling=1, \n",
    "                                                class_weight=None, \n",
    "                                                random_state=None, \n",
    "                                                solver='lbfgs', \n",
    "                                                max_iter=1000, \n",
    "                                                multi_class='multinomial', \n",
    "                                                verbose=0, \n",
    "                                                warm_start=False, \n",
    "                                                n_jobs=None, \n",
    "                                                l1_ratio=None)),\n",
    "                     ])\n",
    "\n",
    "#text_clf_logistic_regression.fit(X_train, y_train)\n",
    "#predicted = text_clf_logistic_regression.predict(X_test)\n",
    "#print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPssUdcgv2b2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf_naive_bayes = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB(alpha=1.0, \n",
    "                                           fit_prior=True, \n",
    "                                           class_prior=None)),\n",
    "                     ])\n",
    "\n",
    "#text_clf_naive_bayes.fit(X_train, y_train)\n",
    "#predicted = text_clf_naive_bayes.predict(X_test)\n",
    "#print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGNTn87Sv2b9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LinearSVC(penalty='l2', \n",
    "                                       loss='squared_hinge', \n",
    "                                       dual=True, \n",
    "                                       tol=0.0001, \n",
    "                                       C=1.0, \n",
    "                                       multi_class='ovr', # treina n_classes classificadores tipo one-vs-rest\n",
    "                                       fit_intercept=True, \n",
    "                                       intercept_scaling=1, \n",
    "                                       class_weight=None, \n",
    "                                       verbose=0, \n",
    "                                       random_state=None, \n",
    "                                       max_iter=1000)),\n",
    "                     ])\n",
    "\n",
    "#text_clf_svm.fit(X_train, y_train)\n",
    "#predicted = text_clf_svm.predict(X_test)\n",
    "#print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5InP74wsv2cD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "text_clf_knn = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', KNeighborsClassifier(n_neighbors=5, \n",
    "                                                  weights='uniform', \n",
    "                                                  algorithm='auto', \n",
    "                                                  leaf_size=30, \n",
    "                                                  p=2, \n",
    "                                                  metric='minkowski', \n",
    "                                                  metric_params=None, \n",
    "                                                  n_jobs=None)),\n",
    "                     ])\n",
    "\n",
    "#text_clf_knn.fit(X_train, y_train)\n",
    "#predicted = text_clf_knn.predict(X_test)\n",
    "#print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txDVjVvlv2cH",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Árvore de Decisão\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "text_clf_decision_tree = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', DecisionTreeClassifier(criterion='gini', \n",
    "                                                        splitter='best', \n",
    "                                                        max_depth=None, \n",
    "                                                        min_samples_split=2, \n",
    "                                                        min_samples_leaf=1, \n",
    "                                                        min_weight_fraction_leaf=0.0, \n",
    "                                                        max_features=None, \n",
    "                                                        random_state=None, \n",
    "                                                        max_leaf_nodes=None, \n",
    "                                                        min_impurity_decrease=0.0, \n",
    "                                                        min_impurity_split=None, \n",
    "                                                        class_weight=None, \n",
    "                                                        presort=False)),\n",
    "                         ])\n",
    "\n",
    "#text_clf_decision_tree.fit(X_train, y_train)\n",
    "#predicted = text_clf_decision_tree.predict(X_test)\n",
    "#print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4sCZFM6Dv2cL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_clf_rf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier(n_estimators=100, \n",
    "                                                    criterion='gini', \n",
    "                                                    max_depth=None, \n",
    "                                                    min_samples_split=2, \n",
    "                                                    min_samples_leaf=1, \n",
    "                                                    min_weight_fraction_leaf=0.0, \n",
    "                                                    max_features='auto', \n",
    "                                                    max_leaf_nodes=None, \n",
    "                                                    min_impurity_decrease=0.0, \n",
    "                                                    min_impurity_split=None, \n",
    "                                                    bootstrap=True, \n",
    "                                                    oob_score=False, \n",
    "                                                    n_jobs=None, \n",
    "                                                    random_state=None, \n",
    "                                                    verbose=0, \n",
    "                                                    warm_start=False, \n",
    "                                                    class_weight=None)),\n",
    "                     ])\n",
    "\n",
    "#text_clf_rf.fit(X_train, y_train)\n",
    "#predicted = text_clf_rf.predict(X_test)\n",
    "#print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "721Nj5cqv2cY",
    "outputId": "8bdd21ab-0bb4-4a5f-bdf3-ef0ef03862ab",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rede Neural (encapsulado dentro da classe base do scikit-learn) conforme dica do Prof. Boltd\n",
    "# %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class RedeNeural(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" Classe customizada com base no sklearn \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\" Estrutura da rede neural adaptada de Kowsari (2019) \"\"\"\n",
    "        nFeatures = X_train.shape[1]\n",
    "        nClasses = np.unique(y_train).shape[0]\n",
    "        nNos = 256\n",
    "        nCamadas = 4\n",
    "        dropout=0.5\n",
    "        \n",
    "        # O TensorFlow 2.0 ainda não suporta a matriz esparsa do scikit-learn:\n",
    "        # Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>\n",
    "        # Então converti X_train para array\n",
    "        X_train = X_train.toarray()\n",
    "\n",
    "        self.model = tf.keras.models.Sequential()    \n",
    "\n",
    "        self.model.add(tf.keras.layers.Dense(nNos,input_dim=nFeatures,activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "        for _ in range(0, nCamadas):\n",
    "            self.model.add(tf.keras.layers.Dense(nNos,input_dim=nNos,activation='relu'))\n",
    "            self.model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "        self.model.add(tf.keras.layers.Dense(nClasses, activation='softmax'))\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])       \n",
    "\n",
    "        self.model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # O TensorFlow 2.0 ainda não suporta a matriz esparsa do scikit-learn:\n",
    "        # Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>\n",
    "        # Então converti X_test para array\n",
    "        X_test = X_test.toarray()\n",
    "        \n",
    "        predictions = self.model.predict(X_test)\n",
    "        return np.argmax(predictions,axis=1)\n",
    "\n",
    "\n",
    "text_clf_ann = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', RedeNeural()),\n",
    "                     ])\n",
    "\n",
    "#text_clf_ann.fit(X_train, y_train)\n",
    "#predicted = text_clf_ann.predict(X_test)\n",
    "#print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tpxzFwberoX"
   },
   "outputs": [],
   "source": [
    "# Classe avaliadora de performance dos classificadores (com base no exemplo do Prof. Boldt)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class PerformanceEvaluator():\n",
    "    \"\"\" Classe avaliadora de performance dos classificadores (com base no exemplo do Prof. Boldt) \n",
    "        Utiliza validação cruzada para o treinamento.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.kf = KFold(n_splits=5)\n",
    "    \n",
    "    def score(self, clf):\n",
    "        scores = []\n",
    "        for train, validate in self.kf.split(self.X_train):\n",
    "            clf.fit(self.X_train[train],self.y_train[train])\n",
    "            scores.append(clf.score(self.X_train[validate],self.y_train[validate]))\n",
    "        return np.mean(scores), np.std(scores)\n",
    "    \n",
    "    def treinar(self, clfs):\n",
    "        print(\"TREINAMENTO (com validação cruzada) - - - - - -\")\n",
    "        print(f'{\"\":>20}  Média \\t Desvio Padrão')\n",
    "        for name,clf in clfs:\n",
    "            score_mean, score_std = self.score(clf)\n",
    "            print(f'{name:>20}: {score_mean:.4f} \\t {score_std:.4f}')\n",
    "\n",
    "    def testar(self, clfs, X_test, y_test):\n",
    "        # Testa os classificadores em dados de teste (não vistos no treinamento)\n",
    "        print(\"TESTE- - - - - - - - - - - - - -- - - - - - - -\")\n",
    "        for name,clf in clfs:\n",
    "            score = clf.score(X_test, y_test)\n",
    "            print(f'{name:>20}: {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "DLuIWQkIerod",
    "outputId": "d477d3fa-6464-43bc-f870-270768c1855c"
   },
   "outputs": [],
   "source": [
    "# Avaliação de todos os classificadores\n",
    "\n",
    "clfs = [\n",
    "    ('Logistic Regression', text_clf_logistic_regression),\n",
    "    ('Naive Bayes', text_clf_naive_bayes),\n",
    "    ('SVM', text_clf_svm),\n",
    "    ('KNN', text_clf_knn),\n",
    "    ('Decision Tree', text_clf_decision_tree),\n",
    "    ('Random Forest', text_clf_rf),\n",
    "    ('ANN', text_clf_ann),\n",
    "]\n",
    "\n",
    "newsgroups_pe = PerformanceEvaluator(np.array(newsgroups_X_train), np.array(newsgroups_y_train))\n",
    "safety_alerts_pe = PerformanceEvaluator(np.array(safety_alerts_X_train), np.array(safety_alerts_y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset SAFETY-ALERTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ur0Mo2pUjOyU",
    "outputId": "4a6996b4-2264-4855-8c2e-951d8a2f0b23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREINAMENTO (com validação cruzada) - - - - - -\n",
      "                      Média \t Desvio Padrão\n",
      " Logistic Regression: 0.2591 \t 0.1472\n",
      "         Naive Bayes: 0.1909 \t 0.1560\n",
      "                 SVM: 0.4712 \t 0.2084\n",
      "                 KNN: 0.4333 \t 0.2226\n",
      "       Decision Tree: 0.4470 \t 0.1220\n",
      "       Random Forest: 0.3121 \t 0.2087\n",
      "                 ANN: 0.0515 \t 0.0422\n",
      "TESTE- - - - - - - - - - - - - -- - - - - - - -\n",
      " Logistic Regression: 0.2500\n",
      "         Naive Bayes: 0.1500\n",
      "                 SVM: 0.6500\n",
      "                 KNN: 0.4750\n",
      "       Decision Tree: 0.4500\n",
      "       Random Forest: 0.3000\n",
      "                 ANN: 0.0750\n",
      "Wall time: 6.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Treina os classificadores usando validação cruzada na base de Alertas de Seguranca\n",
    "safety_alerts_pe.treinar(clfs)\n",
    "\n",
    "# Testa os classificadores em dados de teste (não vistos no treinamento)\n",
    "safety_alerts_pe.testar(clfs, np.array(safety_alerts_X_test), np.array(safety_alerts_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 20-NEWSGROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "YcXyZTVNjLD7",
    "outputId": "1f4abd17-6eb3-4347-8729-ea18186c0665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREINAMENTO (com validação cruzada) - - - - - -\n",
      "                      Média \t Desvio Padrão\n",
      " Logistic Regression: 0.8940 \t 0.0057\n",
      "         Naive Bayes: 0.8393 \t 0.0122\n",
      "                 SVM: 0.9264 \t 0.0028\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a6116d25102a>\u001b[0m in \u001b[0;36mtreinar\u001b[1;34m(self, clfs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{\"\":>20}  Média \\t Desvio Padrão'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclfs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mscore_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{name:>20}: {score_mean:.4f} \\t {score_std:.4f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a6116d25102a>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, clf)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[0mscore_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    350\u001b[0m                                                tokenize)\n\u001b[0;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 352\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Treina os classificadores usando validação cruzada na base 20NEWSGROUPS\n",
    "newsgroups_pe.treinar(clfs)\n",
    "\n",
    "# Testa os classificadores em dados de teste (não vistos no treinamento)\n",
    "newsgroups_pe.testar(clfs, np.array(newsgroups_X_test), np.array(newsgroups_y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[Qualificação] Classificação 20newsgroups.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
